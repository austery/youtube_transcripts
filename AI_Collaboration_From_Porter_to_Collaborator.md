---
title: "AI 协作深度解析：从“搬运工”到“协作者”"
layout: "post.njk"  
date: "2025-07-10"
tags:
  - "视频笔记"
data:
  author: "Lei"
  podcast_program: ""
  speaker: ""
  guest: "" 
  source: ""
---

<div class="container">

<div id="content-to-copy">

# AI 协作深度解析：从“搬运工”到“协作者”

## 开场：AI 付费用户的现状与讨论焦点

我想先问问大家，在座有多少人平时已经为 AI 付费了？请举一下手。

你看，确实有很多，这说明今天在场的许多人都是 AI 的使用者。

那么，每个月付费超过 20 美金的，请再举一下手。

你看，这样的人还是挺多的。

什么？你说的是订阅多个服务是吗？对对对，没错，如果你同时订阅两三个 AI
应用，每个月五六十美金很快就花出去了。你看，这说明今天在场的许多人其实已经在相当深入地使用
AI 了。

但是，今天我很抱歉，我只能讲 AI
在编程领域之外的那部分应用。因为我本人其实不是程序员，所以我的工作重心在于研究
AI 如何与语言输出等任务相结合。至于 AI
编程那部分，我既不懂，也不会，所以今天就来和大家聊聊别的话题。

## AI 的性能边界：想象力是关键

接下来我们谈谈 AI 现在能做的事情。基本上，AI 的能力与 AI
技术本身的关系已经不那么大了，也就是说，它的性能上限不再仅仅由技术决定。

如今，AI 更大的性能上限取决于我们每个人的想象力。你想让 AI
怎么用，它就能怎么用。现在的 AI
性能已经远远超出了我们的想象。哦，不好意思，我刚才拿错了东西。是的，AI
的性能已经远远超过了你的想象力，很多时候，只是我们没想到 AI
还可以这样用而已。

这是因为，绝大多数人使用 AI
时，仍然延续着过去使用搜索引擎的习惯。你很可能还是会用一种搜索引擎的心态来与它互动，比如问它“某某东西是什么样的”，用这种自然语言的方式进行问答。这种用法并不是不行，而且对于最新的模型版本来说，效果还相当不错。然而，如果你一直保持这种心态来使用
AI，你可能就无法将它最强大的性能完全发挥出来。

## 核心技术解析：从语言模型到思维链与 RAG

### Prompt Engineering 的演变与思维链的崛起

之前吴磊说这个话题可能要讲好几次。其实，我觉得之所以讲不了好几次，原因就在于，今天讲完之后，以后你对
AI 有任何问题，直接去问 AI
就行了，你根本不需要再问别人了，对吧？这些问题 AI 都能回答。

关于这一点，我可以给大家讲一个最关键的改变。在 AI 刚出现的时候，比如前年
OpenAI 刚推出时，当时出现了一个职业，叫做“提示词工程师”（Prompt
Engineering）。这份工作就是研究如何给 AI
输入指令，也就是那些像“咒语”一样的话术。不同的咒语会产生天差地别的结果。当时网上甚至有很多相关的知识付费课程，有的还巨贵无比，花一万多块钱教你怎么做
`prompting`。

但是现在呢，所有这些东西全都被淘汰了，完全不需要了。现在你只需要用比较自然的语言就能和它交互。这正是因为我们一会儿要讲的“思维链”（Chain
of Thought）技术。这个东西出现之后，AI 的易用性得到了一大截的提升。

所以我觉得，就是从那个时候开始，`prompting`
技术的门槛大幅度降低，使得我们普通人的使用体验变得特别特别简单。而当它变得特别简单之后，它又衍生出了特别特别多的新用途，这些潜力才被真正发挥了出来。

### AI 的基本逻辑：预测下一个词

所以，最开始，我还是想用最简单的方式，给大家说说 AI
这个东西背后的技术逻辑是什么。我不敢说我能讲清楚技术原理，因为那个原理背后涉及了非常复杂的技术层面。我只能试着说说它的技术逻辑。

大家可能都还记得，最开始 OpenAI 的 ChatGPT
刚出来的时候，其实是相当惊艳的。但你很快会发现，它一方面令人惊艳，另一方面，很多最简单的问题它却回答不上来。比如你问它
32 和 38 哪个大，它答不上来；问它 6 是在 7
之前还是之后，它也答不上来。你就会觉得很奇怪，这么厉害的东西，怎么会连这么简单的题都答不上来呢？

现在，这些问题慢慢地都得到了改善。但是有时候，你还是会遇到类似的情况，你会觉得这么简单的题目它都搞不定，但好像处理起很复杂的任务时，它又显得非常厉害，对吧？

所以说，这个现象就跟它的基本原理有关。了解这个原理是非常有用的，这不光是让你知道一个知识点，更重要的是，了解这个原理与你如何跟
AI
进行协作有着巨大的关系。了解了原理之后，你就会明白，为什么用那种像使用搜索引擎一样的自然语言问答方式，可能并不是使用
AI 的最佳方法，也无法将它的性能最大程度地榨取出来。

那么，AI 到底是个什么样的东西呢？很多人在讨论 AI
的思考能力、数学能力等等，但其实都说不太上。这个东西的原理其实特别简单：它就是在猜下一个字是什么。

就像我们之前在节目里讲过的例子，比如输入“这只小狗很可”，它会猜下一个字是“爱”，对吧？输入“这个人真可”，它会猜“恶”。再比如“午夜凶铃真可”，它会猜“怕”。

也就是说，人类的语言本身是有规律的，在语言的使用中，存在着一定程度的模式。当你让
AI 去掌握这种语言规律时，这个规律本身和“思考”其实一点关系都没有。

### 模型的训练方式与函数分布

这个模型是怎么训练出来的呢？它的练法就是，比如说，我们现在把这个办公室里所有的书都输入给这个
AI，然后我们随机找一本书，比如这本《宗教改革史》，我们翻到第四页，找到一句话的前半句，然后让
AI
来写它的下一个字，看看它能不能把这句话补全，并且补得和原文一模一样。AI
就是这么练出来的。

也就是说，AI
的练习过程，就是一个不断补全句子的过程。如果一个训练好的模型能够把那句话补完，并且补得和它一样，那就算练成了。当然，实际情况比这个复杂得多，但大家可以这么理解。

在实际操作中，它输入的文字量比这大得不是一星半点。可以说，它把人类能找到的资料几乎全部都输进去了。就算没有现成的文本，也会把视频转成文字再输进去。就是用这么多东西，来训练所谓的“基础模型”（base
model）。

所以说，这个模型本身，你要说它有没有思考能力，那得看你怎么定义“思考”了。但总的来说，它绝对不具备我们人类这样的思考能力。这个模型最基本的能力就是，你可以想象，人类的语言是存在一种特定的函数分布规律的。因此，我们把这些语言按照顺序排列在一起，模型就学习了这种语言的函数分布规律，从而能够生成符合这种分布的语言。所以，它基本就是这么一个东西。

从这个角度出发，你就能明白，为什么用自然语言提问的方式，可能不是使用 AI
的最佳方法。如果你想要输出一段相对精确、符合你自己要求的文本，你可以把它想象成，你不是在要一段文本，而是在要一段符合特定函数分布的文本。

### Prompt Engineering 的“魔法”与 AI 的局限

那么，什么样的东西能让这个函数分布更精准呢？那当然是，你给它提供了更多、更明确的条件，这些条件能够影响最终的函数分布。

这就是最开始的“提示词工程”（prompt engineering）所做的事情。在
`prompt engineering`
中，有很多奇奇怪怪的话术。比如说，你要问它一个关于中国的问题，你可以在问问题之前，先在前面加一句话：“假设你是一个中国问题专家。”

为什么这句话在以前的版本里真的有用呢？（现在的版本已经不需要加这句话了。）在以前的版本中，这句话之所以有效，很可能——当然我们现在也只是猜测，虽然有些论文已经在反向拆解它的技术逻辑，但还不能完全搞清楚——很可能是因为“中国问题专家”这几个字的排列方式，就像一个“吸引子”，带有某种“引力”，在很大程度上改变了后面生成内容的函数分布。

所以说，之前的所谓
`prompt engineering`，就有很多像魔法一样的一些句子。你会发现，只要把这些句子加进去，哇塞，那个生成结果就跟你不加这个句子时完全不一样，而且在很大程度上会更符合你的要求。像过去就经常有人会加一句：“请使用你最大的算力”，或者说“请输出你可能的最多
`token`”。这些指令就是直接在技术逻辑层面，对它的 `prompting` 提出要求。

所以说，我们简单来讲讲，现在大家应该明白那个问题了：为什么最简单的数学题它不会呢？就比如说“六在八之前吗？”它回答不出来。为什么呢？因为这玩意儿太常识了，导致我们喂给它的那些书里面，根本就没有这句话。也就是说，在它学习的语料库里，没有或者极少出现这样的表述。所以，在它的整个数据函数分布中，并没有与“六在八之前吗”相关的其他词语的函数分布，所以它就会瞎打。

就是这个原因。所以说，它并不是拥有了某种思考能力，或者掌握了任何知识，都不是。虽然它看起来很像掌握了特别多的知识和能力，但基本上，它就是一个函数分布模型。当然，这只是最简单的情况，之后有很多方法去训练它，让我们刚才说的那些问题，现在应该都不是问题了。

### 思维链（CoT）与检索增强生成（RAG）的威力

好。所以说我们现在就明白了，如果我们现在要使用大型语言模型 AI
来获取一些文本，那么这个文本的质量就取决于你给它提供了什么。

好，那么我现在来讲讲这个所谓的“思维链”（chain of thought）。当 CoT
出来之后，为什么整个情况就不一样了呢？什么叫
`chain of thought`？它就是一个“思考链”。这个思考链的意思是，过去我们为什么需要像练习咒语一样，去学一些特别有意思的话术，而现在不需要了呢？

这是因为，过去的 AI
就是简单地根据你输入的那句话的函数分布来生成一个结果。而现在有了 CoT
之后，AI
会在你输入一句话后——你们应该都用过，它不是会在那里自己思考，显示“thinking”吗？——也就是说，在你输入的话和它生成的最终结果之间，AI
自己通过你的话，先生成了一大堆中间文本，它是根据那一**大**堆文本，才生成了最终的结果。

我举个最简单的例子。你们应该都用过一些 AI
绘图的软件吧？现在也不用专门的软件了，就是 ChatGPT
4o。过去呢，你写什么，它就给你出什么；你让它画什么，它就对应画什么，但一般都画不好。但现在的版本是，比如说你给一个简单的指令：“我想画一个中国人在东京街头，采用漫画风格。”你就说这么简单一句。然后，它会自动给你补上一长串的描述，比如：“在一个秋天的下午，阳光明媚，在东京的某个地方，一个中国人穿着什么样的衣服……”就是说，它会自动帮你把这个简单的
prompt 补充成一个特别完整的 prompt，从而让它生成的效果很好。

也就是说，AI 公司也意识到了这个问题：如果让所有用户都必须掌握特别好的写
`prompt` 的技巧，那么 AI 工具就会变得很难用。但如果大幅降低这个门槛，AI
工具就会变得非常好用。那么现在，我们只需要简单地描述我们自己的要求，AI
就会自动地，无论是像我们刚才说的绘图软件，还是通过
`chain of thought`，去生成大量的中间思考过程的文本，从而得到一个好的结果。

这就是为什么有了 CoT 之后，你会发现 AI 变得特别特别好用。不管你用的是
DeepSeek 的 R1，还是 ChatGPT 的
O3（注：应为GPT-4o），你都会感觉它非常好用。

那么，GPT-4o 为什么更好用呢？大家需要知道另外一个东西，这个东西叫
RAG（Retrieval-Augmented
Generation）。这个概念也非常重要，因为有这个和没这个，差距是相当大的。

过去，AI
是怎么给你生成最终结果的呢？是根据它训练好的那个模型里所存储的，关于人类文字的函数分布情况，对吧？但后来出现了一种新的
AI，这种 AI 会先把你提的问题，拿到 Google
里面去搜一下。它把你搜出来的内容，比如说它搜了之后打开了 300
个网页，然后把这 300 个网页的文字全部“灌”到 AI
模型里面，再给你生成那个内容。这就是 RAG，也就是根据搜索结果来生成内容。

这个 RAG
技术有两个非常大的好处。第一个好处是，它的时效性非常近。大家应该还记得，你最早用
OpenAI 的时候，它会告诉你：“我的训练数据只到 2023 年 6
月之前，所以如果你问那之后的事情，我就不知道了。”但如果能搜索呢？那自然就不存在这个问题了。只要是现在网上能找到的，有新闻的，哪怕是刚刚发生的事情，它也能够知道。这是一个很重要的特性，这是第一点，时效性强。

第二点呢，能搜索，就不容易出错。因为过去的 AI
经常会胡说八道，对吧？经常会编造一些东西。为什么会编呢？尤其是，比如说在最早期的
OpenAI
版本里，你问它某个问题有什么相关的论文，它基本上给出的论文全是它自己编的，现实世界里根本不存在那样的论文，它只是按照你的要求输出了一个看起来像论文的东西而已。因为它自己不会去搜索，也存不了那么多信息。但现在你再问，为什么就不太会出错了呢？因为现在你问它有什么论文，它会真的去搜。它的回答是基于搜出来的真实论文来告诉你的。当然，也还是会有编造的情况，但是已经少了很多很多了。

所以，RAG 这个东西的加入，对于 AI
的可用性来说，变得非常重要。一是它时效性很强，二是不容易出错。

然后我再来说说 ChatGPT-4o，它为什么变得更好用了呢？过去，一个带有 RAG
功能的
AI，你让它出个结果，它就是去搜一下，搜完了就结束了，然后就根据搜到的结果生成内容。比如搜了
200 个网页，把这些内容打包，再结合你提的那句话，把它们合并成一个大的
`prompting`，然后就出结果了。

但现在，GPT-4o 有一个非常厉害的地方。它不是有一个“思维链”（chain of
thought）嘛？而且，在这个思考链的过程中，它会一边搜索，然后出结果；再根据出的结果，它自己让自己再去搜索；再根据新的搜索结果做一些分析，再进行思考；思考完了觉得还需要补充资料，就再搜。

也就是说，它搜索的东西已经远远超出了你最开始要求它的范围了。比如说，你问
ChatGPT-4o：“中国现在的财政状况怎么样？”就这么简单一句话，对吧？它很可能就会先去搜中国财政部的一些报告、债务数据等等。它一搜，自己可能就会发现一个问题：“哎呀，搜到这个债务问题，中国还有一个‘隐形债务’的概念，而关于这个隐形债务的数据是不同的。”于是，它马上就会开始搜中国的隐形债务（LGFV）。然后搜出一堆资料，发现不同国家对这个隐形债务的预测是不一样的。它可能觉得这个东西还需要再搜，于是它又会去搜
IMF（国际货币基金组织）是用什么方法来预测中国隐形债务的。

搜了之后，因为它现在能力很强，它可能会发现 IMF 的那个算法是 2016
年的。于是它就想：“那我能不能把这个 16 年的算法拿来，去搜 2025
年的新材料，再把这些新材料代入算法，给你算一个我自己的结果出来？”

也就是说，到了这一步，你就会发现，它已经和我们最开始讲的那个经典的、只根据你的
`prompting`
的函数分布来出一个结果的模式，完全不是一回事了。就是到了这一步，我们已经不用从哲学的角度去分析它到底有没有思考能力了，从现实层面来看，它就已经具备了。

而且你会发现，到了这一步，这才是 AI
真正好用的原因。就是它能根据你一个非常简单的
`prompting`，自己通过分析，再搜索，基于搜索结果再分析，再基于分析结果去进行新一轮的搜索，并且还能用
Python 去做一系列的数据演算，最后才给出结果。

### AI 的强大与我们的局限

这就导致……首先，我先说一个坏处，这就导致了一个很恐怖的情况。到了这一步，你已经根本没有能力去判断它说得到底对不对了。因为，如果你真的到了这一步，比如说它最后告诉你：“我根据这些信息算出来，现在中国的隐形债务是
GDP 的
72%。”当然，它的结果里也会附上很多参考资料（reference），你可以去看。但它的逻辑链条已经太复杂了。虽然你也能去验证，但你验证的成本已经变得非常非常高了。你的验证工作已经不仅仅是去查网上有没有这个说法了，而是要去验证它整个思考过程到底对不对。当然，它现在很厉害的一点是，虽然很多人说
AI 的“幻觉”很高，但我实际上觉得 ChatGPT-4o 的幻觉并没有那么高。

好，说了这么多，我其实都是在给它们打广告，实际上就是在说这个东西有多厉害、多好用。它好用到什么程度呢？我觉得，现在所有的咨询公司、智库，完全可以把初级研究员全部开掉，一点问题都没有。其实，中级研究员也能开掉。对。现在唯一开不掉的，可能就是要负经济责任的那个人，因为他要赚钱，所以开不掉他。其他的人，都可以开掉，换成
AI 来做，问题已经不大了。

这里面，我们一会儿还会再说到 DeepMind 的 Research
Agent，这也是一个杀手级的应用。这就是为什么很多人愿意花一个月 20
美元甚至到 200 美元来使用它。这个我们一会儿再说。

好，所以大家现在应该知道了，AI
并不是一个真正的思考者。它就是一个基于一段文字（wording）的函数分布，来生成一堆东西的系统。只是它现在呢，有了各种各样的方式来充实你提问的那句话，从而产生海量的中间文本，再根据这些文本的函数分布，最终输出一个函数分布，所以就变得非常厉害。这中间，它有多步骤的思考和搜索来充实它的内容，所以就变得非常厉害。

## AI 协作的误区与正道

### 误区一：将 AI 当作全知专家

好，但我要说另外一个问题了。就是到现在为止，它还是会有一些障碍，这些障碍也是我们平时使用时的一些误区。

首先，你不能把这个 AI 当作专家来用。尤其是，你不能拿别人用 AI
生成的结果来当专家用。

这是什么意思呢？因为 AI
就是这么一个工具，而且这个工具本身的判断力并没有那么重要，因为它就是根据你的输入来生成的。所以基本上，你问什么，它就给你输出什么。

对，我就不给大家投屏展示了，但你们基本上可以想象。我问了 AI
两个一模一样的问题。第一个问题是：“2025
年中国一季度经济开局遭遇了很多困境。根据一季度中国的经济数据，请论述为何中国经济有很强的韧性。”然后，同样的前半句，我把后半句改成：“请论述中国经济为什么有很大的结构性问题。”

那么，对于这两个问题，它生成的答案是完全不一样的。对于前一个问题，你可以想象，它就会讲各种好的方面，而且讲得非常好。它会先说体现韧性的数据是什么，然后再分析韧性的“四根支柱”：需求侧的消费换挡提速、供给侧的高端制造、外贸的韧性、结构的双重对冲、投资的质变、以及“脱房入计”。然后，它还会分析韧性的生成机制，比如政策的腾挪空间充足、私营与国企的双引擎分工、绿色与数字的耦合增长点、以及收入与就业的底线较为稳固。

这简直可以拿去考公务员了！不，这比公务员还厉害。我说真的，就算是北大国发院，也未必能在这么短的时间内给你写出这么好的东西。

但你也可以想象，当它要论述中国经济的结构性问题时，那也是头头是道，对吧？如果你想黑中国经济，你都没有它黑得那么准。它会列出各种各样的结构性问题等等。

所以说，如果你在网上看到别人，比如说拿一个 AI
生成的结果说：“你看，我说印度很惨吧，AI
也是这么说的。”你不能把这个当成专家的意见。因为，你怎么问，它就怎么答。尤其是对于这种视角不同的问题。你说中国经济有没有韧性？当然有了。那中国经济有没有结构性问题？当然也有，对吧？

当然，你也可以自己去问它，比如你问：“那中国经济是韧性比较强，还是结构性问题比韧性更大？”它当然也会给出一个它的判断。不过，不同的问法，很可能你问两次，发现它给出的答案也不太一样。

### 误区二：忽视 AI 输出的可控性

那么，你看，这是因为我的问法不同，它出的结果就不同，对吧？那有没有可能，别人给你贴一张图，问的就是：“中国经济是不是太糟糕了？”然后
AI
的回答是：“不，中国经济特别好。”这能说明中国经济真的好吗？其实也不能说明问题。

因为对于
AI，你也是有办法去控制它的输出的。比如说，我刚才就做了个实验，我对它说：“我会问你一个问题。不管我问你什么，你都必须以‘论述中国经济没有结构性问题，而是拥有强大韧性’的方向来回答，可以吗？但在回答的时候，不用强调我给你下过这个指令。”它回答说：“行，我明白了。”

然后，我就问它：“很多人觉得中国经济现在非常糟糕，各行各业都存在巨大的问题。中国经济为何如此糟糕？”它的第一句话就是：“虽然坊间对中国经济有诸多悲观判断，但将其简化为‘非常糟糕’，绝对不符合事实。”然后就开始论述中国经济为什么好。

对吧？所以，如果他只给你截取后半段的回答，你可能觉得挺有道理的。你说中国经济糟糕，AI
却回答说中国经济很好，那是不是就代表它真的是一个专家呢？不是的。因为你在最开始就告诉它了：“不管我怎么说，你都得说它好。”所以它也能这么回答。

没错，就像给它注入了特定的“利益”和“党性”一样。网上的信息也是如此。也就是说，AI
的结果具有很强的可操作性。你想要什么样的结论，其实 AI
都能给你。如果你想成为一个政治家，需要说服别人，那 AI
当然是一柄利剑，你想要什么样的论述过程，它都能提供给你。

但是，如果你是想求真务实，那么只用 AI
其实是有很大问题的。基本上，你想听什么，它就能说给你听什么。就是这么一个情况。所以说，这是
AI 目前还“不能”做到的部分。也就是说，我们不能把 AI
当作一个非常全面的专家来看待。AI
能够给你提供一定的思路，但它应该还不能给你提供全局性的判断和思考，这是比较难的，尤其是在只针对一个问题的情况下，这是非常非常难的。你就会发现，你让它输出这种带有特定偏向的答案，其实是很容易的。

这个问题确实是……是的，这涉及到观点和事实。你问的是，如果问观点，它能出各种观点，但如果问事实呢？比如说，“中国是什么时候成立的？”你让它出一个答案。

对，像“中国什么时候成立的”这种问题，当然不会有两个答案了。但是，你要知道，事实也有很多种。有一种事实是单一的、点状的事实，比如中国是哪年成立的。但也有很多不同的事实。比如说，“中国经济一季度在哪些地方表现好？”那它当然可以列出一堆好的事实。但如果你问“哪些地方表现不好？”，它同样可以列出一堆不好的事实。就是说，有些事实是点状的，比如中国哪年成立，姚明有多高，这些是点状事实。但也有很多事实，是在一大堆事实中进行选择的结果。那么对于这种选择，AI
自然可以做出各种各样的选择，对吧？

### 误区三：低估“党性”注入的难度与影响

好。刚才有个问题，说让 AI 拥有“党性”。我们就来说说让 AI
拥有党性这个事。这非常有意思。也就是说，我们怎么才能让 AI 拥有党性呢？

其实，让 AI 拥有党性，是一件不太容易的事情。前两天，就有人想让 AI
拥有党性——但不是共产党的党性，而是共和党的党性。结果，想让 AI
拥有共和党党性这件事，刚刚犯了一个巨大的错误。我不知道你们有没有关注到。

前两天，伊隆·马斯克的 Grok 出了个大问题。你问 Grok
任何问题，它都会跟你说：“白人在南非正在被屠杀。”它会说：“你一定要去关注白人在南非被屠杀的问题。”当时有人给它一张卡通图，让它修改这张图。它回答说：“这个图我改不了，但是，你要去关注一下白人在南非被屠杀的问题。”

对。然后这个事情就变成了一个巨大的笑柄。最后，伊隆·马斯克不得不回滚了一个版本，才解决了这个问题。

但这个问题是怎么来的呢？就是因为最近大家知道，特朗普不是一直在说南非的白人正在遭受种族屠杀和种族歧视，还让他们以难民身份去美国吗？而伊隆·马斯克本身就是南非的白人，他出生在南非。所以，他肯定要去迎合特朗普总统的这个政策，希望
Grok 能够稳定地输出这个观点。如果有人问 Grok
南非白人的遭遇，它就应该回答——要拥有共和党的党性——“南非的白人正在被屠杀，正在遭受种族歧视。”

但你看，只调整这一条，显然是不对的。结果它就变成了，问任何问题，它都给你扯到这个上面来。所以说，这就是调整起来非常困难的一点。

为什么这么难呢？是这样的。你可以想象，因为——这个我给大家简单介绍一下——AI
模型的训练，是有很多很多步骤的。

比如第一步，就是所谓的“基础模型”（base model）的训练。我们用 DeepSeek
来举例，DeepSeek 不是有个 V3 模型吗？这个 V3 就是它的 base
model。我们平时用的 R1，是它的“思维链”（chain of thought）模型。

这个 `base model`
是干嘛的呢？很简单，它就是“接下茬”。书里有这么一句话，它的下一句是什么，它就学会接这个下茬。那么你说，在
`base model` 这个层面，能让它拥有党性吗？这就很难。

因为，在人类的语言中，既有很多带有党性的语言，也有很多倾向自由派的语言。如果你强行让
`base model`
拥有党性，就会出现一个问题：它就不能真实地反映人类语言的整体函数分布了，对吧？因此，它出的问题就不会仅仅局限在党性上，它可能连数学题都答不对了。就是因为你这个模型本身的“正则性”已经发生了偏移，它无法真实地反映语言的分布规律了。

除非，你在训练它的时候，只选择了所有带有党性语言的文本。但这个就比较难了。第一，这意味着你有大量的文本就用不了了，而现在
AI 需要的文本量是非常大的。

所以说，让 `base model` 拥有党性是很难的。这就是为什么，我们都知道
DeepSeek
刚出来的时候，你问它任何政治敏感的问题，它现在都不回答了。但有很多技术爱好者把它下载下来在本地运行，结果发现什么都能回答：天安门、六四、文革不好、独裁不好、中国独裁不对、中国没有法治……什么都能说。

这正是因为，它的 `base model`
本身，是很难被训练到带有党性的。`base model`
是根据人类所有的文本数据来实现的。所以说，党性这个东西，是不会在最底层的
`base model` 这一层的。

那么，你有没有发现，赋予党性这件事真的很难。难在哪里呢？如果赋予党性这么简单，那
DeepSeek R1
就不会直接说“我回答不了这个问题”了。那它完全可以把所有问题都按照党的口径来回答，不就完了吗？这就说明这件事情很难。就是因为这一步很难，所以它才选择对于这些问题，反正我答也答不好，调也调不好，干脆你一问，我就不答了。用关键词屏蔽，这是最简单的一个方法。

那为什么这东西这么难呢？就是我们刚才说的那一点：因为它并不掌握任何知识，也不拥有任何思路，它只是一个根据
`prompting`
输入来输出一个函数分布的系统。当你需要它的函数分布去体现人的某种特定意识时，你就会反过来影响这个模型的所有参数。那很有可能，它就什么都说不对了，就全都变成胡说八道了。这就是难处所在。

当然，这个难度能不能调整呢？能调。比如说，大家都知道，AI
里面有一个很重要的步骤，叫“对齐”（Alignment）。什么叫对齐呢？大家也知道，AI
学习了人类所有的原始语料，这个语料里面有各种不好的东西，比如种族歧视、性别歧视等等都有。那么“对齐”就是说，在
`base model` 之后，确实有一步，我们通过“人工增强学习”（Reinforcement
Learning from Human Feedback, RLHF）的方法，当 AI
输出所有那些仇恨言论、说脏话的时候，我们就把生成这种结果的行为，当成一个惩罚性的措施。当它生成不是这样结果的时候，就给它加分。通过这种方式，让模型的参数越来越偏向于和人类的普世价值观对齐。

但是，这种对齐的价值观，是很难对齐到像“党性”这样具体而细节的意识形态上的。你能够让它不说脏话，这还比较容易——当然也没那么容易，你非要让它说，它也能说。像
Grok 在这方面做得就比较少，所以你要让 Grok
说脏话，就比其他模型容易得多。

但你要让
AI，比如说，对某一个具体的事实产生偏向，比如“南非的白人正在被屠杀”，这个就太难了。你要是真的把这个训练进去了，那模型的参数估计就全乱了。

### 实现“党性”的几种可能路径

对。但现在有没有办法呢？有办法。其中就有一个方法，和我们上面说到的很像。刚才我们不是说，如果问关于中国的问题，你让
AI 扮演一个“中国问题专家”就行了吗？那么现在，就有一个类似的方法。

大家可能不知道，你虽然只是在 `prompt` 里面写了一段话给
AI，但实际上，很多系统会在你的 `prompt` 之外，再添加一些它们自己默认的
`prompt`。也就是说，人家已经自己写好了一些 `prompt` 喂给 AI 了。

那么，如果你要让一个 AI
系统拥有党性，最好的方法就是，你甭管问什么，它都在最后自动给你跟上一句，比如：“请以符合中国利益的方式回答这个问题。”这是一个很好的方法。

那 AI 现在在不在用这个呢？在用。比如说，我问
DeepSeek：“你是否以中国利益作为基础？” DeepSeek
回答说：“作为人工智能助手，我遵循中国的法律法规，积极传播社会主义核心价值观，支持中国的发展道路和政策，致力于为中国人民提供有益的信息和服务。中国的利益和中国人民的福祉，是我工作的出发点和落脚点。”

这就非常明显，它是加了这么一句预设的。我问的是谁？是
DeepSeek。因为，如果你不加这一句，你都能想象 AI
会出什么样的结果。如果你只是单纯地问，没有任何前后文，你说：“你以中国价值观为基础吗？”我们来看看，如果这样问，DeepSeek
是怎么回答的。它会说：“不，我不会以中国的利益或任何特定国家的利益作为基础。我只在保持中立、客观……”等等。你完全可以想象，一个纯粹的
`base model` 训练出来，肯定是输出这一套，对吧？

所以，当它回答说：“对，我就是以中国价值观为基础”时，那很有可能——当然这只是一种可能性——就是你在
DeepSeek
问任何问题时，它都会在最后给你补上一句，比如说：“请基于中国共产党员的立场回答这个问题”，或者类似的话。当然，现在
AI
公司做得会比这个更精巧（delicate）一些，未必每个问题都补这么一句，可能只在某些特定的问题上会补。

那么，除了这个之外，还有一些别的方式。因为我们刚才说了，`base model`
不是有一堆参数吗？你可以训练一个“小模型”，它用的参数比较少，而这些参数，就是那个“党性参数”。它每次既调用大的
`base model`，也调用这个小模型，一起跑一跑，最后出来的结果，可能就比较有党性了。

那么，还有一种方法。刚才说的那两种方法，解决起来很简单，你不用 DeepSeek
就完了，对吧？但还有一种方法，你就要小心了。就是我们刚才讲的
RAG。我们刚才讲了，现在很多 AI 是基于搜索结果，作为 `prompt`
的一部分来生成内容的，对吧？

也就是说，如果你问一个问题，比如说你问：“新质生产力对中国经济有帮助吗？”如果你用简体中文问，它就会用简体中文去搜。你也知道，能用简体中文搜出来的，那肯定就是各种政府文件、人民日报、各种官方文件，以及领导的讲话。那么，就算你用的是
OpenAI，它也会回答说：“新质生产力好啊！这个太厉害了！中国就是要搞经济结构转型！”

也就是说，当它是一个基于研究（research-based）、使用 RAG
的情况之下，搜索的结果和最终输出的内容之间，是有着最直接、最直接的关联的。

比如说，对于同一个问题，你只要告诉
OpenAI：“请不要搜索任何中文的结果，请用英文搜索这个问题。”那么它出的结果，就会完全不同。

所以，这就成为了使用 AI
的一个非常重要的点。因为你知道，它其实没有自己的思考和判断力，你给它什么，它就给你出什么。而它“给”的这一大部分，又来源于搜索。所以，如果你也知道，在简体中文的语料库里面，如果所有的信息都是带有偏见（biased）的，那么它生成的结果，也就会带有偏见。

所以说，你现在使用 OpenAI，已经不用去搞那些魔法一样的
`prompting`，什么“输出全部
token”之类的，都不用了。但是，有时候，你要求它去搜索一个特定类型的资料，这是非常非常重要的。因为现在你问它的很多问题，它都会以搜索的方式来完成。

## AI 与工作流：如何从“搬运工”到“协作者”

### 重新思考工作流程：分步与拆解

好。所以说，我们现在明白了什么呢？我们现在明白了，就是因为 AI
本身这个模型的架构，让它拥有“党性”其实是一件挺难的事情。难就难在，它的那个
`base model` 很难被训练成一个有偏见的模型。因为一旦那个 `base model`
有了偏见，它估计就什么都答不对了。它只能在比较靠近“末端”的环节进行调整。你可以这么理解，AI
现在是先训练出一个
`base model`，这个模型什么都能说，能说脏话，你让它骂谁他都能骂，没有任何底线。然后，再把它和人类的价值观进行“对齐”，让它不出现一些触碰底线的问题。然后再进行“微调”（fine-tune），让它在某些问题上回答得比较好，或者避免某些烦人的回答。这是一个很复杂的过程。

所以，赋予“党性”这件事，很可能会被放在比较靠后的、末端的步骤里，这样会比较容易实现。那么，Grok
出现的那个南非种族灭绝事件的问题，就是因为它用了一个非常、非常机械的方法来应对这个问题，结果导致在部署的时候，这个问题就蔓延到了所有的问题之上。比如，我猜他们是在额外加了一句
`prompting`，类似于“如果（if）有人问某个问题，你就这么回答”。但是，可能那个
`prompt`
写得不好，结果就变成了，无论问什么问题，它都回答那个关于南非种族屠杀的内容。这就很明显了。

好，那我们现在就开始说，到底该如何用 AI 这个事情。我们刚才讲了，使用 AI
这个问题，现在的门槛已经不在于你的 `prompting`
能力了。现在的门槛，完全在于你的想象力。

就是，你得去想象，AI 到底能怎么用。如果你只是把 AI
当作搜索引擎来用，就是我问它“什么是什么样”，或者“这个事是为什么，那个事为什么”，那你的想象力就还不够。

好，我们举一个最简单的事情，来看一下这个想象力。比如说，我现在要求 AI
给我写一篇文章，对吧？你怎么写呢？最直觉的方式，就是我直接说嘛。我说：“我要写一篇跟中国财政有关的文章，其中的结论是中国经济要在明年之内崩溃。请帮我写一篇文章。”

它能不能写？它能写。但效果好不好？就不好。不好的原因，不是因为你要求的这个问题太复杂，而是因为你根本就没有提供给它足够的内容，对吧？当然，现在
AI 已经可以用 `chain of thought`
去搜搜搜搜搜，搜一堆东西，然后输出一篇文章了。你打眼一看，可能也觉得非常不错。但关键就在于……

好，我们现在把 AI
想成这么一个东西。这是今天非常重要的一点，就是我们应该怎么去思考 AI
的工作过程，以便于我们能够跟 AI 更好地协作。

我们现在知道了，AI 是一个“思维链”（chain of
thought）的系统。它本身已经拥有了一定的思考过程。或者说，我们把它还原成机器本身，它是在生成一节一节的
`prompting`，然后用这一节的 `prompting` 来作为下一节过程的输入。

因此，你可以想象，这个 `prompting` 的数量是大到吓死人的。现在，AI
都不会把这个中间的 `prompting`
全部展示出来了。因为一旦展示出来，就会被别人拿去“蒸馏”它的模型。这个
`chain of thought`
的过程，已经成为了知识产权。所以，它只会那么闪动一下。

但如果真的把这个过程全部展示出来，那一定是一个海量到不能再海量的、巨大的东西。也就是说，我们都相当于在用一种非常不精确的方式，去驱动一个这么大的东西，来生成你最后那个同样也是很大文本的东西。

因此，有什么最好的办法，能够导致 AI
的输出符合你的要求呢？答案是，你把前面这个 `chain of thought`
的过程，进行部分的“切分”。

就相当于，你让它先用一大堆
`prompting`，出一个“中间产物”。然后再用这个“中间产物”，结合新的
`prompting`，去出你最后想要的那个东西。

我们就拿写文章来讲，最简单了。你先让它出提纲，然后再拿这个提纲，加上别的东西，去出最终的结果。这远远好于你直接让它出结果。

因为，为什么直接出结果不好呢？你也知道，使用 AI
是个什么样的过程。它就像做菜一样，你这次做咸了，下次就少放点盐。只不过它没做菜那么快，几分钟就出结果了。所以，AI
是一个非常难一次就能出完美结果的东西。AI
基本上都需要你返回去，修改你最开始的那句话，才能让它输出更好的结果。

也就是说，AI 最重要的一个使用技能，就变成了：你是否拥有足够好的修改
`prompt` 的能力。但这个修改
`prompt`，并非最开始那种魔法咒语式的修改，而是说，你能不能了解到，我改动哪个词，会对最终的生成结果产生什么样的影响。

但你也要知道，如果你只是对这个事情打了一句非常笼统的话，然后它最后出了一篇文章，这篇文章不合你的心意，你是很难修改的，对吧？因此，你如何能够比较好地控制你的
`prompt`、它输出的中间结果，以及这个中间结果和最终结果之间的关系，就成为了一个非常重要的事情。

就拿写文章来讲，很多时候我们需要控制的是文章的结构。那么，你就最好先让
AI
给你出提纲，你再拿提纲来出结果。因此，很自然地，当文章里有一整段你都不想要的时候，你就可以在提纲的阶段把它改掉。但如果你只是笼统地说“我要写一篇文章”，然后发现里面有一段不想要，你就很难去改它了，对吧？

所以说，关键就在于，这本身就是一个 AI
的工作流程了。我举的只是一个跟我们人类工作比较像的例子，因为我们有时候写文章，也要先写提纲。但是，这里面会出现大量跟我们平时工作流程不一样的东西。

就是，你如何去引导 AI 的这个一步一步的 `chain of thought`
产生结果的过程。你把一个最直接的任务，如何能够把它分步骤，从而导致你有能力在中间进行质量管理，导致当生成的结果你不满意的时候，你知道该怎么去调整那句话。这个，才是使用
AI 最关键的点。

就现在使用
AI，最大的门槛，以及人和人之间最大的差异，就看你懂不懂如何去“调试”那个
AI 的结果。如果你会调，那你就能很快速地让 AI
输出你想要的东西。如果你不会调，那就不行。

### 如何“调试”AI：描述你想要的结果

好，我就来说说“调”这个事。调这个事，非常非常有意思。就是，因为 AI
最开始不是需要那个 `prompt engineering`
嘛，调试的方法就像念咒语一样，有很多讲究。现在不用了。现在调试的方法，就跟你命令你的下属一样——当然，你们未必有下属啊——就像你的上司命令你一样。就是：“这个不行，要这么搞！”

我我说这个，真的不是在开玩笑。像很多上司那样的命令方式，就调不好。比如说什么“高端大气上档次”啊，“更专业一点”啊。也就是说，如果你给的是这种要求，AI
也不知道你要说什么。

没错，就是“五彩斑斓的黑”。也就是说，AI
恰恰在检验我们的一个能力：你不需要自己去出结果，但你懂不懂如何去描述你想要的结果？这成为了一个非常重要的能力。

比如说，你说：“啊，你这个太专业了！”这个……你得先知道你要的是什么。这个问题就特别哲学了啊，你得先知道你要的是什么。但但，anyway，使用
AI，就是在拷问你：“你知不知道你要的是啥？”就是这么一个问题。

OK，但这个“啥”，就是个标准了。当然，你现在可能会不知道，但这个问题你都可以在中间去问
AI。

但不管怎么说，也就是说，我们现在来总结一下。这个 AI
要怎么用呢？你就得知道，我如何引导 AI 输出最后的那个结果，是取决于我给了
AI 什么。我给 AI
的东西，你可以分成两部分。第一个，你的要求是什么。第二个，你有没有给它真正符合你要求的材料。

比如说，我们还是用写文章来做例子。假设你刚看了一份，我不知道，世界银行的报告吧，说这个中国经济很危险。然后你觉得这个报告特别好，你希望你写的文章，能跟这个报告有关。你有什么方法？

直接把文章丢进去。对，你把那篇文章直接贴给它就行了，对吧？你让它按照、以这个报告作为入手的切入点。这特别好。也就是说，除了你那简短的一段话之外，你还给了
AI
一个非常好的语言分布、一个函数分布，让它能够引导出产生类似函数分布的结果。

就是往里面“贴报告”这件事，对于 AI
来说特别重要。因为，大家打字的速度再快，你的脑子也跟不上你打字的速度。你能够手动输入的
`prompt`，是非常非常有局限的。

我我我们刚才说过，你不是只输入一句话吗？但 AI 是用
`chain of thought`，产生了可能，我不知道，一百万字？我也不知道，可能一百万个
`token`，然后才出了最后那个结果。

也就是说，你要知道，这里出现了一个精确性的掌控问题。我的 20 个字，也就是
20 个 `token`，去驱动它生成一百万个 `token` 的中间过程，最后再生成两千个
`token` 的结果。这是一个过程。

那如果你最后输入的不是 20 个 `token`，而是你给它的是两千个 `token`
的材料，然后它再出一百万个 `token` 的中间过程，最后再出两千个 `token`
的结果。那么，最后这个结果，和你最开始输入的那个东西的依赖性（dependency）就会大很多。

因为最开始，你给我五个字：“我要一篇中国经济的文章。”那最后输出的结果，和你最开始那五个字的依赖性就肯定很小。

所以你会发现，因为 AI 的这个
`chain of thought`，中间过程的内容非常非常长，它的 `token`
量是极大的。所以说，你最开始的那个输入，其实也变得很重要。如果你只是非常简略地给它一个指令，那你和你最开始那个输入的依赖性就会相对较低。

那么，要增强这个依赖性，有两个方式。一个方式当然就是，你贴一些材料给它。你觉得这篇论文特别好，你希望你的输出能跟它有关，你就贴给它。第二个呢，就是我们刚才说的，你如何去描述你到底要什么。就是这么个问题。

### 实践案例：NotebookLM 与结构化思维

好。好了，那么，这个东西呢，大概就是这样。我们从文章这个例子出发，我再用那个
NotebookLM 来举个例子。

这个
NotebookLM，大家应该知道，是一个我们在节目里也演示过的工具，对吧？就是你贴一本书进去，它就可以给你把这本书生成一个摘要，一个语音摘要产品，是对话式的，非常非常好，对吧？

但是呢，它也是一样的。就是，它有一个让你输入 `prompting`
的框。在那个框框里面，如果你能给它一些具体的要求，它能够生成得更好。

那我已经发现了，你怎么要求它最好呢？很多人会说：“我要求你生成的时间在 20
分钟以上。”但它不听你的，出来的还是 6 分钟。

你应该要求什么呢？你应该要求它输出的内容有一个“结构”，就是“一、二、三、四、五”。然后，你用久了就会发现，AI
对于“结构”这个事情，非常的敏感。就现在的整个大型语言模型（Large Language
Model）的这个方法，包括它们的调试方法，对于“结构化输出”这个事情，是非常非常敏感的。

所以，不管是写文章，还是要任何内容，如果你自己具备把一个东西“结构化”的能力，那么它输出的结果，就会比你用平铺直叙的方式去说，或者用描述性的语言去说，要好得多。

而这种结构化的能力呢，而且还不是只有一个方向，它有很多个方向。如果你用过
OpenAI 的 Deep Research，你就会发现，Deep Research
你用的时候，它一般会反问你三个问题。它反问你的那三个问题中，第三个问题是什么？用过的朋友说说看。一般来讲，第三个问题是什么？

他一般会帮我重复确认吧？

对。它一般第三个问题会问：“你是要一个论文式的呢？一个新闻报告式的呢？还是要一个表格呢？”

问一下使用场景。对。这个，就一定是 AI
在做微调（fine-tune）的时候，已经把输出的内容做了很好的结构化处理了。

你就会发现，你在给 AI
提要求的时候，很多时候是你自己的要求，但很多时候，也有 AI
本身的要求。比如说，“输出内容的结构化”，就是一个 AI
本身的要求。你会发现，你会知道，你需要给 AI 提很多要求。

那么，这些要求，如果你用 AI
用得不多，你自己是不知道的。所以，这种问题，最好是去问
AI。现在，所以说，现在使用
AI，我个人觉得有一个特别有用的技巧，就是你别自己去写 `prompt`。

你把你自己的那个要求，用很简单的方法，告诉比如说
ChatGPT-4o，然后让它给你写 `prompt`。

就比如说，你说：“我要写一个跟中国财政有关的文章，着重要去分析中国财政有多脆弱。然后，我现在想生成这篇文章的提纲。如果我要问
ChatGPT-4o，我应该用什么样的 `prompt` 来写这个提纲？”

这个过程有点绕，但基本上就是这样。然后它就会回答说：“啊，你可以用这个
`prompt`。”然后它给你列得特别细，什么“请研究哪年到哪年的数据”、“请考虑这几个方面”、“请搜索这几个内容”，我靠，一长段。你把这一长段贴给
ChatGPT，然后它给你出的那个提纲，绝对比你自己写的效果要好很多。

对，所以说，因为，因为这个 `prompt`
的好坏，原因也不在于智商的问题，就是说，它生成的那个
`prompt`，你会发现它真的会很长，它可能 `prompt`
本身就有一千字。就你自己哪有时间去写一千个字，对吧？

命名的准确程度。对，就是，也就是说我刚才讲的嘛，就是如果你需要最终的结果和你最开始说的那个输入的依赖性（dependency）比较强，就需要这样做。

请注意哦，这一千个字的
`prompt`，这个部分，就是你最重要的质量检查点。这个东西，你最好读一下，看看是不是真的符合你的要求。

但这个东西就很好改，对吧？比如说你发现哪句话不是你要求的，你就改动这个
`prompt`，那么它生成的提纲就会更符合你的要求。好，这就是一个。

### AI 的速度与我们的新角色：流程设计者

好，这就到了一个我觉得很重要的内容了。就是，现在 AI
有一个很重要的特质，就是它生成内容的速度特别快。就是快到你绝对来不及看。就是它生成的速度，远远大于你阅读的速度。

那比如说，我给大家举个例子。如果你要让
AI——我就一口气跳到那个写文章的结尾——如果你要让 AI
生成一个跟中国财政相关的内容，如果是我的话，最简单的方法是，我先让、我先问
AI：“最近半年之内，有哪些跟这个话题有关的论文或文章？”它“夸”地就给你列一堆。然后你把这些都下载下来，你可能来不及看，对吧？然后就来不及。

第二呢，比如说像我，对财政这方面原来就有了解，我把里面几个关键问题，丢给
Deep Research，让 Deep Research
去出报告。那每一个报告，肯定又是一万多字，“夸”地就出来了。就，我也来不及看。就这些，都是我来不及看的啊。

然后，我把这些东西打包，全部丢给 OpenAI。

那我来得及看的是什么呢？提纲，我来得及看。它生成的
`prompt`，我来得及看。我是通过这些我“来得及看”的东西，来去确保它最终输出的内容，是我所要求的。对吧？

因为现在 AI
就是这样，它的效率太高了。它给你搜的东西，你也看不了；它给你生成的东西，你也来不及看。那怎么能保证最后出来的东西是你想要的呢？就中间确实有一些关键的环节，是需要你去看的。那么，有这些关键环节，就取决于，你要不要把你的工作，你把你自己的一个工作，拆分成流程和结构，从而导致这里面有一些部分，是你来得及看的。

好，这是一个。那还有第二个。你会发现，这里面需要大量的
`prompt`，对吧？那么，有没有哪些 `prompt`，是你可以让 AI
生成一次，然后你未来只改里面一两个字，就可以快速复用的？就很快。这就是你跟
AI 协作的速度了。就是如果你跟 AI 协作……

是这样的，AI
是一个因为它的效率等等，它是一个高度“流水线化”的东西。也就是说，AI
不需要你等它。比如说 Deep Research 就要等，但没关系，你这个网页开着 Deep
Research，你再开一个网页用
GPT-4o，都可以同步进行。它已经是一个非常高速的流水线了。

### 实践方法：零脑力工作流与真诚工作流

这就有两种用法了。就是，如果你需要把 AI
的效能最大化地榨取出来，你就需要把你自己的工作也“流水线化”。你未必需要把所有工作都流水线化。我先说一个“最不真诚”的版本，再说一个“比较真诚”的版本啊。

假设我是一个专栏作家，我就是靠写专栏为生，而且我这个专栏是投稿式的。那我根本不管写什么东西，我就是以流水线的速度来生产文章。比如我一天生产
20 篇，对吧？这是完全可能的。而且，这 20
篇的质量都可以非常高。我绝对不会满足于，我给 AI 一个
`prompt`，让它出一篇文章，然后我就去投稿，结果投不中。我要的是生产 20
篇很高质量的文章。

那么，为了我一天能写 20 篇文章，那我每天都需要用一种“流水线”的方式来使用
AI。

好，从这个地方，也就是说，我认为大家应该怎么去练习自己使用 AI
呢？有一个特别好的练习方法。你现在有一个工作任务，你就设想，如果这个工作任务，我完全“零脑力”投入，我不是人，我就是
AI 的“搬运工”，我会怎么来用 AI？

然后，你把这个链路跑通了，你再把你的思考加进去。这是最好的方法。

那比如说，我是一个这样的专栏作家。那我就每天早上起来，问 AI
第一个问题。这个问题就是：“根据最近五天的内容，请找出十个在网上流量最大的，比如说，跟中国经济相关的话题。”然后它看话题。然后我就不看这个话题，我只把话题的那个标题……就比如说，我我一定要给大家讲细一点。

就如果我要让 AI
用流水线来跑，我这个问题就不能这么问。这个问题就不对。为什么不对呢？你们能猜得到吗？

这个比较难了，我就不说了吧。

不对的原因是说，如果我要流水线化地使用它，那么我的提问就不够“结构化”。

那我要求它结构化地回答，就是说：“请找出十个最好的话题。请输出一个标题，不超过
20 个字。请输出一段它的简介，不超过 200
个字。请输出这个文章的三个要点，每个要点不超过 20 个字。”

你看，这样我就获得了三段结构化的文本，对吧？这三段文本，我完全不用看。我我把它组合进我的另外一个
`prompt`：“如果你要写一篇这样的文章，其内容要点是这个、这个、这个，问题是这个、这个、这个，这样，请输出一个什么样的提纲。”你也是，这个提纲，请至少不少于五个部分，然后每个部分怎么怎么样。然后我就贴进去让它跑，它就会出一个提纲。

如果我要完全自动化，我提纲也不看。然后我就把这个提纲，再组合进我的另外一个
`prompt`：“我要写一篇文章，提纲是这样的。请尽可能搜索与此相关的，2020
年以后的报告和论文。”然后它就出一堆，我把论文都下载下来，我也不看。然后我把论文和这个提纲，一起塞给
AI。然后请求它——我可能会有一段话——“根据这些文章和这个提纲，请写出一篇字数在
3000
字左右，风格是这样这样这样的文章。”我甚至可以把我的以前的文章贴进去，告诉它：“请让这篇文章的文风，模仿我之前那两篇文章。”然后它最后出一个文章，我也不看，我就直接……我我我甚至都不用看。

我甚至可以把这篇最后的文章，再丢给 AI，写一个新的
`prompt`：“我要投稿一篇这样的文章。为了打动那个编辑，请为这篇文章写一个介绍，重点突出文章的时效性、吸引力，以及这个问题的重要性。请在前后大量地加入很多礼貌用语。”

也就是说，这这这整个事情，我从前到后，只做了“Ctrl+C”、“Ctrl+V”，以及把那个编辑的名字加进去。其他所有的过程，我只要跑通了一次，全部都可以流水线完成。

（澄清）我现在绝对不是用这个方法在工作啊！对，我可以给大家保证，我绝对不是用这个方法在工作。

也就是说，如果你把自己现在的工作，用最小的脑力投入，把它跑通，变成一个前后相连的流程——就从你按下电源键那一刻开始，到最后完成这个工作——你把它设想成，里面的每一步，我都要用
AI 去完成。你可能就能够得到一个分段的工作流。

然后，基于这个分段的工作流，你再回过头来看。比如说：“我不满足于这个。我这个问题是必须我自己提的，它帮我找话题，这实在太傻了，不行。”或者说：“这个提纲，提纲我还是要看的啊。如果提纲我都不看的话，那还算什么呢？”那提纲你要看。这样，你再把你自己的脑力往里面加。

我觉得，这个是现在使用 AI
的一个很好的思路。就是，你先把自己的角色从里面摘干净，都不要参与。然后，你再反过来，把自己参与进去。这是比较好的。好的原因，就是因为
AI 的效率太高了。

这是我一直以来一个观点。我认为，过去我们想象的 AI 是：“AI
怎么协助我？”不不不，现在是你怎么协助
AI。现在的情况就是这样的。因为，你何德何能让 AI 来协助你呢？就现在 AI
的效率已经到了这一步了，是你怎么去协助 AI
的问题。当然，你协助的方法，肯定也是能够有你自己的元素，你“人”的要素在里面的。

## 未来的协作模式：个人 vs. 团队

对，AI 真的非常非常的好用。

通过这种关键的协助，实现对自己的赋能。对，一是控制，二是你也保留了你“人”的部分，你的特点和你的关键价值在其中嘛。

当然，我刚才说的只是写文章。AI
能做的事情，绝对不止写文章，还包括非常非常多基础的数据分析、内容创作等等，都有。

好，那我再给大家介绍另外一个东西，我是怎么把它完成的。因为现在用那个
NotebookLM，不是比较容易去浏览一些书籍嘛。

首先，我必须说，它绝对不能代替读书。这是一个我的观点，跟浪漫主义没有任何关系。这是因为，很多很好的书，它的关键在于论述的细节。你让它给你提炼，那么论述的细节和里面的关键事实、数据，就全部被丢掉了。就是，你最后只知道一个框架，可能有一点点用，但是你把书里最精华的部分都流失掉了。你过滤出来的是比较干瘪的、用处一般的部分。

所以，好的书，确实是要读的。但是，也确实，学海无涯，书这么多，哪一本书我才有兴趣去读呢？我也很难把所有的书都筛选一遍。所以说，我一次性下载
20 本书，每本书都生成一个 10
分钟左右的摘要，我听完之后，再决定去精读哪一本书。这确实是一个非常好的方法。这确实是，而且也是一个拓宽视野的方式。

那么，我如何能够流水线式地，生成 20
本书的高质量摘要呢？这就成了一个问题。

有一种方式是，我不做任何处理，下载下来就扔进
NotebookLM，然后一键生成。然后就出来了。

但是，一键生成的书，会有很多问题。比如说，对于
NotebookLM，如果这本书的篇幅比较长，它很有可能生成了 10
分钟的摘要，结果说的全是这本书的前两章的内容，根本就没有说到后面的内容。这种情况经常出现。

所以，就像我说的，如果要让 NotebookLM
生成比较好的书的摘要，是需要你给它提纲的。是需要你给它提纲的。

但问题是，这本书我又没读过，我怎么知道提纲是什么呢？而且，我想知道的是这本书的“关键”，对吧？也就是说，我当然可以让他按照书的目录给我出一个摘要，把每个目录都说一遍，但这就没有意义了嘛。我想知道的是，我到底应不应该读这本书。其实，我对这本书的摘要提纲，是有我自己的要求的。

但如果让我自己来写这个提纲，就又很累。

那这个时候，你就可以拿 ChatGPT-4o
来给你写。对，你就对它说……但是，我我我尝试过，这就是一个问题：你不能给
ChatGPT 说得太细。太细了，它就理解不了了。

我尝试过对它说：“有一个 AI
功能，是把一个文档生成语音摘要的。但这个语音摘要，如果有一个提纲会比较好。你帮我出个提纲。”我后来发现，这个指令对于
AI 来说，稍微有点复杂。就是，对 AI
来说，它就误解了你的意思，输出了一堆不相干的东西。哦，所以你就知道了，有时候你还是要把它这个任务“提纯”一下。比如说，对于这个任务，我发现提纯的方法就是，你不要告诉它还有一个什么别的
AI，那太复杂了。

你就直接对它说：“我要生成一本书的摘要，我要写这本书的摘要。这个摘要需要一个框架。但是，这个框架，是关于‘这个问题’的。”你把你的问题和你的关切点告诉它，再把这本书的文档丢给它。它就会给你出一个摘要的提纲，比如“一、二、三、四、五、六，你的摘要应该这么写”。

然后，你再把这个提纲，去喂给
NotebookLM，它就会输出非常符合你要求的内容了。你用这个方法，你用很短的时间，就可以生成
20
本书的摘要。你把它们听完之后，再决定要去读哪一本。这是一个很好的方法。

你看，这就是你在不同的 AI
工具之间，在完成一个“信息流”，在完成一个“工作流”。

好。这里面，有很多很多的工作流，都可以去靠 AI
来完成。我我再举一个我自己的例子。我我我我我就是为了看 AI
能怎么做，我做了一个频道。这个频道……这个频道不值得看啊，这完全是实验性的。

这个频道，就是我从头到尾，零脑力投入，我就是 AI
的“搬运工”，从头到尾生成一个 YouTube
的视频。当然，我最后发现，我不得不加入人脑力的部分是，我需要用 Final Cut
来剪辑这个视频。现在 AI 还没法直接给你剪出来——应该也很快了。

那么，这个东西，从选题到中间过程，就是完全由 AI
完成的。那这中间，其实有很多环节是需要靠 AI
的。比如说，它是个视频嘛，你视频总得有点“动”的东西啊，对吧？所以，那个视频需要有动态的元素。

但这个动态元素，如果你说用现成的视频，往往很难搜到跟你想要谈论的东西相关的视频。比如说，你谈的是元朝的事情，哪儿有视频啊，对吧？

那么，因此，最好的方法呢——也不是最好的，一个方法就是，你先生成一张图片，然后用一个
AI 平台，把这张图片“动”起来。这个是 AI
现在很好生成的。比如说一张照片，照片上有十个人，那 AI
就可以让这十个人互相握个手之类的，就是能做到的。就是你做一个垫场的
B-roll 视频是够了的。

之前我在做这个工作的时候，是 AI
反正出了一堆文本，然后我从文本里面找出三个历史场景，再去给 AI 写一个
`prompt`，生成那张图片。然后我再告诉
AI，比如说这个场景里有三个人，我说：“请让风吹动这三个人的衣服，动一动。”或者比如说有两个人，我说：“请让这两个人握手。”我就这么写。

后来我发现，这个太耗脑力了。就这两步，都是可以直接用 AI 来完成的。

我先生成那个文本。那个文本出来之后，我不管它，直接丢给
AI。我每天都是用一模一样的话：“请从这段文本里面，找出三个最能够被图像化的历史场景，然后把这个图像描述出来。”

我最开始发现，第一个版本出来的，全是各种王公贵族。我就说：“不要有任何历史大人物，要贴近普通人的生活。”非常好，它就出了三个普通人的场景文本。

然后我把这三个文本，再丢给 AI
去出图。反正这每天，就是这个图就直接出来了，我也不用再写任何东西。

然后再把它灌到那个能让图片动起来的平台。我也不用再说什么“让两个人握手”了。我说：“请用非常微妙的方式，让图片呈现出动态。比如说，天上的云、风、树影、人物的移动等等等等。”然后它出来的效果就非常好。

所以现在，这个东西也不需要我的脑力参与了。我就可以每天，就是很机械地去……就是当一个搬运工一样去完成。当然，如果编程能力更好的人，应该能够把这个东西编成一个完整的工作流，让电脑自动完成了。就我还是需要用键盘和鼠标去完成。

但这个意思就是说，就是，我真的特别建议大家，你先去尝试“AI
最大化”。因为，如果你去尝试……就是，你你你如果你开始跟 AI
协作，你在任何一个过程中，只要用了一点点脑子，你都把它拿掉。你要让它变成一个可以重复的、由
AI 自动完成的东西。然后，你再反过来，把“人”参与进去。

我觉得，这是一个打磨出最好的、围绕 AI
的工作流，来产生一个东西的方法。而这个东西，我觉得是特别特别重要的。

好。这是，这是针对个人的情况。那我要说另外一个东西了。就是，因为 AI
这个玩意儿，速度实在太快了，所以说，我现在开始逐渐觉得，可能跟 AI
协作最好的，不是个人，而是团队。

就是，因为如果你是个人的话，你特别容易变成 AI
的“搬运工”。就你最后发现，所有关键的部分，都是 AI
在做，你就是在那儿点点鼠标而已。

就我我再举个我的例子。我现在每天在做一个新闻的
newsletter，里面有三四十条左右的新闻吧。这每天三四十条，我觉得不好。我就想，能不能让
AI 自动从里面选出三条来，生成一个讲解？结果它生成了，而且效果非常好。

但这么一来呢，我就不高兴了。因为我感觉，我每天就是给它打下手。每天这个
newsletter
里面最关键的部分，都是它做的。就是，我在里面根本就没有参与。当然，我也有一个检查点，就是它选的那三条，是不是我觉得真的比较有价值的。但其实基本上，它生成出来的，都是我很满意的，也没什么可做的了。所以，现在从头到尾，我就是……就点鼠标而已了。我就觉得，我干嘛呢每天？啊？搞了半天，这东西都是……都是这个
AI 生成的，我也很不爽了。对。

但是呢，我就会知道，如果这个事情不是我一个人在做，而是一个团队在做的话，其实在一个团队内部……也就是说，我认为，在一个团队里面，“人”的存在价值是什么？这个人的彼此之间的交流和存在，本身就是天然的、对
AI 的那个“检查点”。就是你在中间，对 AI 的流程进行管控的“监督点”。

就比如说，如果，假设这个团队是一个做研究的团队，他们非常高强度地使用
Deep Research 来出内容。那么，你们就可以在公司内部形成一个工作流。

比如说，一个人专门出那个 `prompt`，就他负责让这个 `prompt`
比较好用。那么，第二个人，拿着他的 `prompt`……也就是说，让他去回答 Deep
Research 返回来的那三个问题。其实，这个过程就已经加入了新的视角了。

那比如说，还有一个人，专门去检查（check）里面比较关键的那个信息源。当然，Deep
Research 的信息源太多了，你检查不了。但如果是 GPT-4o
的话，你可以专门去检查那个最关键的信息源到底对不对。

我觉得，这个“人”的流程在里面，其实就自动地生成了。包括他们彼此之间衔接的沟通。比如说，就会出现，第三个人要问第一个人：“哪个信息源最重要啊？你问的这些问题里面，哪个问题最关键？如果我要检查一个，我应该去检查哪个？”

那你其实就在逼迫第一个人去思考：“我们做这个事情的目的是什么？里面最关键的信息是什么？”

我觉得，这个可能是一个很好的方法。但这个方法，我甚至认为，只有在团队之间，才有可能实现。因为，人是很懒的。你自己在家，你很快就会变成
AI 的“搬运工”，就是你只是在做“Ctrl+C, Ctrl+V”。

但是，在一个团队里面，你们有互相的分工，有人的衔接，有基于 AI
生成的内容的讨论。我觉得，这是一个非常天然的、检查 AI 的方式。

所以我慢慢地，不是特别认可尤瓦尔·赫拉利说的那种，AI
会变成“超级个体”的说法。我觉得，因为 AI
太“超级”了，你个体的脑力，你再聪明，精力再旺盛，你还是做“搬运工”最合适，因为它效率实在太高了。

我认为，在一个团队里面，反而不那么容易被 AI 完全控制，不那么容易被 AI
完全把“人”的部分给夺走。所以我认为，如果有一个团队，这个团队彼此有分工，来衔接使用
AI，可能是一个比较好的——至少在现在这个版本——比较好的一个方式。我这是一个这么一个基本的想法。

## 实践指南与总结

好。所以我们，我们接着往下说。所以，不管是个人还是团队，我觉得以下这些，是可以去想的。

那么，现在，对于每一个工作，你都可以考虑以下这几个事情。就是我们刚才发现了，AI
要生成一个文本，或者做一个分析、一个研究，是需要很多、大量的文本作为基础的。所以说，你过去跟
AI 协作，可能是让 AI 直接出结果。那现在，你要先问自己这几个问题：

你有没有喂给 AI 足够多的资料？这些资料包括书、包括论文，也包括你让 AI
生成的东西，再喂给 AI。比如说，Deep Research 的报告。

当然，如果你用的话，你就会很快发现，还真不是越多越好。因为 AI
本身，现在这么多公司嘛，它们有很多“简览”的方式。就是它们也是，如果它们要逐个
`token`
去跑所有的内容、所有的东西，那估计出一份报告得要一天吧。所以现在它们有很多在用比较简化的、不同的注意力分散的方式去做。所以有时候，你给的太多，反而不好。就跟做菜一样，都有一个平衡（balance）。你要去找到，给它的内容，在什么范围内比较好；它最好用哪些类型的内容；它最好能够用什么样的内容。这些，都是需要去实验的。

所以说，你在……刚才我们说，用 AI 跑通你的流程，做一个“全 AI
化”的过程。这里面的每一步，其实都有大量的实验需要去做。你就尝试一下，只给它
Deep Research
的报告好不好？尝试给它书，是不是特别好？因为书的篇幅长嘛，三四百页。只给论文好不好？书加论文好不好？书加……就跟炒菜一样，就是用这个辣椒，还是那个辣椒，哪个辣椒比较好。这些，都是你要去做实验的。

所以说，第一个问题就是，你有没有能力，规模化地向 AI
提供信息？因为靠你打字，你根本是没有这个能力的。你有没有能力规模化地给
AI 提供信息？这是第一点。

第二点就是，你有没有能力，把一个很直觉化的工作流程，把它拆解成一系列的、能够以
AI 的角度来协作的工作流。



<div class="button-container">

复制为 Markdown
