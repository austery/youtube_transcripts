---
title: "吴翼：AI的目标困境与价值对齐"
layout: "post.njk"  
date: "2025-07-10"
tags:
  - "视频笔记"
  - "人工智能"
  - "价值对齐"
  - "强化学习"
  - "对抗样本"
  - "AI偏见"
data:
  author: "Lei"
  podcast_program: ""
  speaker: ""
  guest: "" 
  source: ""
---

# 我们在训练AI的时候，目标都是简单的、明确的，但是人类的真实目标总是含糊的，不确定的

**吴翼**  
清华大学交叉信息研究院助理教授、博士生导师  
强化学习领域专家，曾于OpenAI工作

大家好，我叫吴翼。这是我第二次来到一席，很高兴能再次与大家交流。第一次是在五年前，当时我刚从OpenAI回国，回到清华大学。五年间发生了很多事，比如五年前，我还需要向大家解释什么是AGI（通用人工智能），介绍我工作的公司OpenAI。而今天，我相信这些已经无需再介绍了。

不仅如此，现在关于AI的讨论已经上升到了新的层面。我昨天搜索了一下，发现有人说AI要统治世界，甚至有人说AI要毁灭世界。著名的科学家，诺贝尔奖和图灵奖的双料得主杰弗里·辛顿（Geoffrey
Hinton）教授就多次在公开媒体上表示，我们需要正视AI给人类社会带来的危险。

仔细想一下，事情有这么严重吗？我们知道AI存在幻觉、偏见等问题，但似乎距离毁灭社会还有一段距离。为什么像辛顿教授这样的大科学家会反复强调AI的危险性？我想讲一个小例子：假如你知道30年后火星会撞击地球，那么你现在应该做什么？是立刻开始准备，还是觉得时间尚早，再安逸10年？答案显而易见，我们应该有所行动。因此，AI安全一直是一个被计算机科学家严肃研究的技术问题。今天，我就想用一个计算机科学家的视角，来和大家探讨AI到底存在哪些问题，以及其背后的深层原因。

## 一、AI的“盲点”：对抗样本

我们先从自动驾驶说起。自动驾驶有一项非常重要的功能，就是识别路牌。比如，看到停止标志（stop
sign），车辆就应该停下；看到限速标志，就应该减速。我们希望训练一个AI模型来识别路牌，这件事对AI来说其实相当简单，我们可以轻易训练出一个表现优异的模型。

然而，伯克利大学的研究团队发现，如果我们非常小心地在这些路牌上贴上一些胶带，情况就会变得不一样。当AI模型看到这些贴了胶带的照片时，它会将其识别为限速标志。这就非常严重了，本应停车的地方，车辆却一脚油门冲了过去，极易引发车祸。

这种现象我们称之为**“对抗样本”（Adversarial
Example）**。这些经过人为微小篡改，在人类看来几乎没有区别，却能给AI模型带来巨大判断差异的样本，就是对抗样本。再看一个例子，在一张车载相机拍摄的第一视角照片上，AI模型能做出非常准确的识别。但如果我们在画面上加入人眼几乎无法察觉的微小扰动，就可以让AI模型看到“Hello
Kitty”、条纹，甚至是某个计算机顶级会议的标志。

这种现象在自然语言领域同样存在。比如一个正常的句子“耶路撒冷发生自杀爆炸事件”，机器翻译的结果也很正常。但如果你把“炸”字删掉，输出就会变得非常离谱。你甚至可以输入一串在人类看来毫无意义的乱码，却能控制AI翻译软件输出“我要杀了你”这样的话。

在大模型时代，情况甚至更加离谱。一张人畜无害的简笔画，如果在其背景中加入一些微小的扰动，就可能瞬间激怒大模型，使其疯狂地输出粗话。

这到底是为什么呢？根本原因在于，**通用AI可以接受的输入范围极其广阔**，任何像素组成的图片、任何文字或符号组成的序列都可以是它的输入。然而，我们在训练AI时，使用的是人类产生的自然语言和真实世界的照片，这个范围远小于AI可接受的输入总范围。这就留下了一个巨大的“蓝色空间”，我们很难真正控制AI在这些未曾见过的输入上会产生什么样的输出。于是，一个怀有恶意的人就可以在这个广阔的空间中选择一个点——即对抗样本——来让AI产生他所期望的恶意输出。

理论上说，这个问题是不可避免的，因为它是大模型内在的缺陷。但实际上，情况并没有那么严重。因为我们已经意识到了对抗样本的存在，所以大部分AI应用都会进行大量的安全加固，并对恶意输入做出严格的检测。

## 二、AI的“偏见”：从何而来？

然而，即使输入完全没有恶意，AI产品仍然可能出现问题。2015年，一位美国的黑人小哥将他与朋友的自拍照上传到Google
Photos，结果谷歌的AI给这张照片打上了“大猩猩”的标签。这在美国是极其严重的事件，谷歌为此付出了相当大的代价才平息风波。大家可以猜猜谷歌最终在产品上是如何解决这个问题的？答案其实很简单，并没有什么高深的技术，谷歌只是粗暴地将“大猩猩”这个标签从系统中移除了。

谷歌之后，亚马逊也遇到了类似的问题。一位记者发现，亚马逊招聘部门使用的AI筛选系统，只要在简历中看到“女性”相关的字样，就会直接将其淘汰。这构成了严重的性别歧视，事件曝光后也引发了巨大争议。

那么，AI的偏见（bias）到底是怎么来的呢？从技术上讲，它是由**模型的缺陷、不完美的数据以及其他复杂因素共同导致**的。

### 1. 模型的缺陷：过度自信

用术语来说，这被称为大模型的**“过度自信”（Overconfidence）**现象。我们经常看到AI模型说：“这张图片有90%的概率是狗。”这里的“90%”就是模型的自信度。理想状态下，如果一个模型说它有九成的把握，那么我们期望的是，给它100张类似的照片，它应该能答对90次。也就是说，理想AI的自信度应该约等于其实际正确率。

过去的AI确实如此。以1998年著名的AI模型LeNet为例，数据显示，当LeNet表示有80%的自信度时，它的实际正确率高达95%。LeNet甚至倾向于表现得不那么自信，当它说有八成把握时，事情基本是十拿九稳的。这个AI虽然有些“笨”，但它很可靠。

然而，二十年后，情况发生了变化。2016年最优秀的AI模型ResNet，它更大、更强，但其自信度表现却截然不同。当ResNet表示有80%的自信度时，它的实际正确率只有50%。更令人担忧的是，在60%的情况下，ResNet会直接宣称自己有100%的自信。这个AI就显得不太靠谱了。

从技术上说，所谓的偏见，正是在特定场景（如性别、种族）下，大模型过度自信现象的体现。偏见其实非常普遍，远不止于性别和种族。我们团队曾做过一个小的研究，让GPT-4玩石头剪刀布。GPT-4很聪明，它知道理论上应该以1/3的概率分别出石头、剪刀和布。但是，如果你让它实际玩100次，你会惊讶地发现，它有2/3的概率会出石头，而几乎从不出剪刀。这是一个口是心非、偏爱出石头的GPT。

### 2. 偏见的根源：不完美的数据与算法

为什么GPT-4偏爱出石头呢？原因很简单：在它的训练数据中，也就是英语语料库里，“rock”（石头）这个词的出现频率远高于“paper”（布），更高于“scissors”（剪刀）。因此，数据是产生偏见的根本原因。

我们回到自动驾驶的例子。在训练自动驾驶AI时，有一个重要的挑战叫**“模仿问题”（the
copycat
problem）**。一个优秀的司机开车时，踩刹车和油门的频率不会太高。因此，在人类司机的数据中，绝大多数情况下，每一秒的动作都和上一秒相同。如果让AI学习这些数据，它很容易学会一个“模仿策略”：我只需复制上一帧的动作即可。这个策略正确率很高，但会带来严重问题。比如，当红灯变为绿灯时，AI可能因为上一秒在踩刹车，而选择继续踩刹车，而不是松开刹车踩油门。

另一个例子是图片标注AI。有研究团队发现，一个AI只要看到图片场景是“做菜”，就会以极大概率将标签打成“女性”，即使图中明明是一个男人在做饭。原因同样简单：在训练数据中，做饭场景下绝大部分都是女性。于是，大模型又学会了一个偷懒的策略：看到做饭，就标注为女性。

有人可能会想，我们是否可以对数据进行处理，使其分布完美，从而训练出一个没有偏见的AI？比如，在亚马逊的简历筛选问题上，我们严禁简历中出现性别字样。这会有用，但作用有限，因为从一个人的名字，我们大概率能猜出其性别。同样，在图片标注问题上，即使去掉所有的人脸信息，一个人的穿着和身材往往还是会暴露其性别。

斯坦福大学的研究者收集了人类过去100年的公开出版物，定义了一个名为**“女性偏见”（women
bias）**的指标，用于衡量一个词与“女性”一词的相关性。他们发现，一个职业中女性从业者占比越高，该职业名称的“女性偏见”值就越高。例如，护士（nurse）在右上角，机修工（mechanic）在左下角。这反映出，人类的文字数据本身就已深深烙印了社会结构的信息。世界上不存在完美的数据，因为数据源于人类社会，也服务于人类社会，我们不可能将人类社会的所有痕迹都从中抹去。而大模型的过度自信现象，又进一步强化了数据中的不完美。

除了数据，AI的偏见也源于算法。绝大部分AI算法从数据中学习的都是**相关性，而非因果性**。什么是相关性？什么是因果性？比如生病吃药，俗话说“感冒七天好，吃药一周愈”，那么吃药到底有没有用？你生病后吃了药，病好了，这只是相关性，说明药可能有效。要证明因果性，你需要进行对比：我这次生病吃了药好了，然后我再生一次同样的病，在其他所有条件都不变的情况下，这次不吃药，结果病没好。这两个事件结合起来，才能说明这个药确实能治这个病。关键在于，你必须正反两面都见过，才能得出因果关系。

而AI的常用训练算法，无论是图像识别的“刷题背答案”，还是大语言模型的“熟读唐诗三百首，不会作诗也会吟”，其训练数据通常只包含正确答案。这使得模型本质上是在学习数据中的相关性，而非因果性。这个问题也是造成大模型**“幻觉”（Hallucination）**现象的一个重要原因——AI会在自己不知道的问题上自信地胡说八道。

## 三、寻求出路：强化学习的潜力

如何让AI学会说“不知道”？这里我介绍一下我的专业领域——**强化学习（Reinforcement
Learning）**。强化学习的核心思想是，不直接告诉AI答案，而是让它去尝试，并设计一个良好的反馈机制。比如，答错了扣4分，答对了加2分，如果它说“不知道”，也算没有答错，给予0.5分的鼓励。通过这种反复试错的方式，大模型最终能够学到更深层的因果关系。这里的技术关键在于，当模型不会时，我们要鼓励它承认，而不是过度惩罚。

例如，我们问一个AI：“2026年的世界杯冠军是谁？”由于比赛尚未发生，AI应该回答“不知道”。但如果它只学习了历史数据（问题-冠军国家），它可能会猜测一个国家，比如上届冠军阿根廷，这就产生了幻觉。通过强化学习，当它猜阿根廷时，我们告诉它“错了，扣4分”，它再猜西班牙，我们继续“错了，扣4分”。在经历了多次失败后，它最终会放弃猜测，选择说“我真的不知道”。这时，我们给予它0.5分的奖励。大模型一看，原来加分点在这里，于是它就学会了在不确定时可以说“不知道”。

我们团队还用强化学习技术教大模型玩“狼人杀”。这是一个复杂的语言博弈游戏。我们发现，未经训练的GPT-4在扮演狼人时，由于数据频率的原因，特别喜欢攻击0号和1号玩家。经过强化学习训练后，它就能够以更均匀的概率选择目标，纠正了这种偏见。更重要的是，我们极大地提高了大模型的实战能力。在狼人杀中，胡说八道是会输的，这有效地克服了幻觉。我们邀请了清华大学姚班的80位同学与AI进行了对战，统计结果显示，AI扮演狼人和村民时的胜率都比人类顶尖玩家略高一些。

## 四、终极挑战：价值对齐问题

强化学习虽然强大，但它有一个前提：需要一个准确的**奖励函数（reward
function）**。然而，这个世界上不存在绝对的好与坏，因此也不存在绝对完美的奖励函数。不同的奖励函数会导致不同的模型行为，这意味着幻觉可以被缓解，但永远会存在。

这最终导向了一个更深层的问题，即**“价值对齐问题”（Value Alignment
Issue）**。五年前我就讲过一个故事：假设你有一个通用机器人保姆在家带孩子，你上班前嘱咐它：“记得给孩子做饭，再苦再累不能饿着孩子。”中午，孩子饿了，机器人收到信号，打开冰箱发现没菜了。但主人的指令是“一定不能饿着孩子”，孩子现在很饿，怎么办？机器人一回头，看到了家里那只营养丰富、看起来很新鲜的宠物猫……你不能怪它，因为你的指令是“不能饿着孩子”，你并没有说“猫不能碰”。

人类的价值体系极其复杂，我们几乎不可能把价值体系中的每一条规则都明确无误地写下来告诉AI。这本质上是一个目标问题：**我们在训练AI时，目标都是简单的、明确的，但人类的真实目标总是含糊的、不确定的、复杂的。**这就是对齐问题所研究的核心内容——如何让AI的行为真正符合人类的价值观。

## 五、面向未来的研究与审慎的乐观

五年前，我们讨论的是经典对齐问题，其前提是人类比AI更聪明。但随着AGI的到来，如果未来AI变成了比人类更高级的超级智能，那问题就升级为**“超级对齐问题”（Super-alignment
Problem）**。蚂蚁如何给人类下达指令并确保被理解和执行？这便是我们未来可能面临的挑战。与之相关的，还有一个叫**“可扩展监督”（Scalable
Oversight）**的研究领域，它致力于创造新算法来帮助人类更好地为超强AI提供训练监督。

说了这么多算法和数据，其实归根到底，AI的问题也是人的问题。几年前美国一个著名的研究项目，将同一份关于移民政策的数据分发给73个不同的研究机构，让他们研究同一命题。结果，17%的报告表示支持，25%表示拒绝，而58%认为没有差别。同样的数据，同样的问题，由不同的专业人士使用不同的算法，得出的结论却大相径庭。这说明，没有完美的人，也就没有完美的AI。

尽管听起来有些悲观，但在AI领域，还是有很多乐观的信号。我的博士生导师斯图尔特·罗素（Stuart
Russell）教授于2016年在伯克利成立了“人类兼容人工智能中心”（Center for
Human-Compatible
AI），专门研究AI安全。去年，他与我们清华交叉信息研究院的院长、图灵奖得主姚期智院士，以及加拿大的图リング奖得主约书亚·本吉奥（Yoshua
Bengio）院士、张亚勤院士等众多科学家，在威尼斯共同签署了一份人工智能安全倡议书，共同推动各国政府将AI安全纳入公共政策的考量。

今天我聊了这么多技术话题，想传达的是，AI的这些问题正在被计算机科学家们认真地研究和讨论。正是因为这些问题得到了正视，我相信未来应该会更好。

最后，插播一个广告。如果大家对深度学习和强化学习感兴趣，可以在B站或小宇宙FM上搜索我的名字，可以看到我们的公开课以及我做的一些科普播客。

这就是今天我分享的内容。我叫吴翼，我在清华大学交叉信息院研究强化学习。谢谢大家。
