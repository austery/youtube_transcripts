---
title: "AI 的正确使用姿势：从 Prompt 到工作流"
layout: "post.njk"  
date: "2025-07-10"
tags:
  - "视频笔记"
  - "人工智能"
  - "Prompt"
  - "工作流"
  - "思维链"
  - "RAG"
data:
  author: "Lei"
  podcast_program: ""
  speaker: "李厚辰"
  guest: "" 
  source: "https://www.youtube.com/watch?v=-a_umjxdcIg&t=1503s"
---

# AI 的正确使用姿势：从 Prompt 到工作流

## 付费用户的崛起：AI 已成刚需

**主讲人:**
我想问一下，大家平时已经为AI（人工智能：模拟人类智能的计算机科学分支）付费的人请举一下手。你看，有很多，所以今天在场的很多人都已经深度使用AI了。付到20美金以上的再举一下手。还是有不少。如果同时订阅两三个服务，一个月五六十美金就出去了。这说明很多人其实已经是在比较深度地使用AI了。

**主讲人:**
但今天很抱歉，我只能讲AI除了编程以外的部分，因为我本身不是程序员，我的工作更关注于AI如何与语言输出等领域结合。对于AI编程那部分，我并不精通，所以我们今天主要聊聊其他方面。

## AI的边界：想象力大于性能

**主讲人:**
AI现在能做的事情，它的性能上限其实跟AI技术本身的关系已经不大了，更大的性能上限在于使用者自身的想象力，也就是你想让AI怎么用。可以说，现在AI的性能已经远远超过了大多数人的想象力，很多时候只是我们没想到AI可以那样用而已。

**主讲人:**
因为绝大多数人使用AI时，仍然延续着过去使用搜索引擎的习惯，用一种“搜索”的心态去与它交互，问它“什么是什么样”这类自然语言问题。这当然不是不行，而且对于最新版本的模型来说，这种方式效果还不错。但如果你仅仅停留在这个心态上，你可能就没有把AI最大的性能发挥出来。

**主讲人:**
所以今天我们讲完之后，后续你对AI有任何问题，直接问AI就行了，基本不用再问人了，这些东西AI都能回答。

## Prompt Engineering的演进：从“咒语”到“思想链”

**主讲人:**
我可以给大家讲一个最关键的改变。在AI刚出现的时候，比如前年OpenAI刚推出时，当时有一个专门的职业叫做“Prompt
Engineering”（提示词工程：设计和优化输入给AI的指令，以获得最佳输出的学科）。这些工程师研究如何输入像“咒语”一样的话，因为不同的“咒语”能产生截然不同的结果。当时网上甚至有非常昂贵的知识付费课程，一万多块教你怎么做prompting。

**主讲人:**
但是现在，所有这些复杂的“咒语”技巧基本上都被淘汰了，不再需要。你现在可以用更自然的语言与AI交互，这很大程度上是因为“Chain
of
Thought”（思想链：一种让AI模型模拟人类思考过程，通过一步步的推理来解决复杂问题的技术）的出现。这个技术让AI的应用性提升了一大截，也大幅降低了普通人使用的门槛，使其变得特别简单。当使用变得简单后，更多特别的用途也就被发掘出来了。

## 揭秘AI核心：它只是一个“猜词”高手

### 语言规律与概率模型

**主-讲人:**
接下来，我用最简单的方式给大家说说AI技术的基本逻辑。我不敢说这是技术原理，因为背后有非常复杂的技术层面，我只能讲一个大概的逻辑。大家可能还记得，最早的ChatGPT刚出来时虽然惊艳，但它连很多最简单的问题都回答不上来，比如“32和38谁大？”或者“6是不是在7之前？”。你会觉得它这么厉害，怎么会答不出这么简单的题呢？

**主讲人:**
这个问题现在虽然好了很多，但有时还是会出现。你会觉得奇怪，这么简单的题目它答不上，但很复杂的任务它又显得很厉害。这其实跟它的基本原理有关。了解这个原理很有用，它不仅是知识，更能帮助你理解如何与AI更好地协作，为什么用传统搜索引擎的方式可能不是使用AI的最佳方法，也无法最大化地榨取其性能。

**主讲人:**
AI其实是这样一个东西，很多人说它拥有思考能力、数学能力等等，其实都谈不上。它的原理非常简单：就是去猜测下一个最可能出现的字是什么。比如我们之前节目讲过，“这个小狗很可”——它会猜“爱”；“这个人真可”——它会猜“恶”；“午夜凶铃真可”——它会猜“怕”。

### AI的训练过程：海量数据与模式识别

**主讲人:**
也就是说，人类的语言存在一定的规律和模式。AI要做的就是去掌握这些语言的规律，但这和我们理解的“思考”毫无关系。那么模型是怎么训练的呢？举个例子，你把这个办公室里所有的书都输给AI，然后随机找一本书，比如《宗教改革史》，翻到第四页，把其中一句话的前半句给AI，让它来写下一个字，看它能不能把整句话补完，补得和原文一样。AI就是这样被训练出来的。

**主讲人:**
一个训练好的模型，就能把那句话准确地补完。当然，实际情况远比这复杂。现实中，输入的文字量大到不可思议，基本上是把人类能找到的所有资料，包括视频转录的文字，都输进去了。这就是所谓的训练“Base
Model”（基座模型：经过海量数据预训练的基础模型，是后续各种专用模型的基础）。

**主讲人:**
所以说，这个模型本身有没有思考能力，取决于你怎么定义“思考”，但它绝对不具备我们人类这样的思考能力。它的基本能力在于，它认为人类的语言存在一种数学上的“函数分布”规律。它通过学习这种分布，就能够生成符合同样分布规律的语言。它基本上就是这么一个机制。

## AI思考的幻觉：为何“聪明”的AI会犯低级错误？

### Prompt的“引力”与上下文的重要性

**主-讲人:**
理解了这一点，你就能明白为什么用自然语言提问的方式可能不是使用AI的最佳方法。如果你想输出一段相对精确、符合你要求的文本，你要知道，这不仅仅是一段文本，而是一段需要精确控制的“函数分布”。那么，如何让这个函数分布更精准呢？当然是给予它更多、更明确的条件来影响这个分布。

**主讲人:** 这就是早期Prompt
Engineering的作用。那些奇怪的“咒语”之所以有效，比如你在问一个中国问题前加上一句“假设你是一个中国问题专家”，很可能是因为“中国问题专家”这几个字排列组合，像一个“吸引子”一样，具有某种“引力”，在很大程度上改变了后续生成内容的函数分布。

**主讲人:** 所以，之前的Prompt
Engineering里有很多像魔法一样的句子，只要加进去，生成的结果就和不加完全不同，并且更符合你的要求。比如过去有人会加“请使用你最大的算力”或者“请输出你可能的最多token（通证/词元：AI处理文本的基本单位）”，这些都是在技术逻辑层面直接要求AI。

### 常识的盲区

**主讲人:**
现在我们可以回到最初的问题了：为什么最简单的数学题AI不会？比如“6在8之前吗？”它为什么回答不出来？因为这个问题太常识了，以至于在我们学习的海量书籍和语料中，很少有这样明确的句子。在它的数据函数分布里，没有足够多的关于“6在8之前”的相关语词关联，所以它就会“瞎打”。

**主讲人:**
这再次证明，它并不是拥有了某种思考能力或掌握了知识，都不是。虽然它看起来像掌握了很多知识，但其本质依然是一个函数分布的预测器。当然，通过后续的训练，这些问题现在基本都已解决。

## 两大技术突破：Chain of Thought 与 RAG

### Chain of Thought (CoT)：让AI学会“思考过程”

**主-讲人:**
现在我们明白了，AI的输出文本取决于你给它的输入。那么，什么叫“Chain of
Thought”（CoT）？它为什么能带来巨大改变？Chain of
Thought，就是一段“思考链”。

**主讲人:**
过去，AI是直接根据你输入的prompt的函数分布来生成一个结果。现在有了CoT之后，AI在生成最终结果之前，会先根据你的问题自己生成一大堆中间的思考过程文本。也就是说，你的问题和最终答案之间，多了一大段AI的“内心独白”。

**主讲人:** 我举个最简单的例子，用过AI画图软件，比如ChatGPT
4.0的人都知道，过去你写什么它就出什么，但一般都画不好。现在的版本，你可能只说一句简单的：“我想画一个中国人在东京街头，漫画风格”。它会自动给你补上一大段详细的描述：“一个秋天的下午，阳光明媚，在东京的某个具体街区，一个穿着特定服饰的中国人……”。它会自动把你的简单prompt，补充成一个特别完整、细节丰富的prompt，从而让最终生成的效果非常好。

**主讲人:**
这说明AI公司也意识到了，如果要求所有用户都掌握高超的prompt技巧，AI工具就不好用。反之，如果大幅降低这个门槛，让AI自己去补全思考过程，工具就会变得极其好用。我们只需简单描述要求，AI会通过CoT等技术自动生成大量中间思考文本，来得到更好的结果。这就是为什么有了CoT之后，AI变得特别好用的原因。

### RAG (检索增强生成)：让AI拥有实时信息

**主讲人:**
那么最新的GPT-4o为什么更好用呢？这就涉及到另一个关键技术，“RAG”（Retrieval-Augmented
Generation，检索增强生成：让AI在生成回答前，先从外部知识库，如互联网，检索相关信息的技）。这个技术的有无，差距非常大。

**主讲人:**
过去，AI是根据它训练时所用的模型（那个包含了人类已有知识分布的静态模型）来生成结果的。但后来出现了一种新的AI，它在回答你之前，会先把你问的东西拿到Google等搜索引擎里搜一遍。它把你搜到的内容，比如300个网页的文字，全部“灌”到模型里，再给你生成最终答案。这就是RAG。

**主讲人:**
RAG有两个巨大的好处。第一，时效性极强。你可能还记得，最早的OpenAI模型会告诉你，它的训练数据只截止到2023年6月，之后的事它不知道。但能实时搜索后，只要网上能找到的新闻，哪怕是刚刚发生的，它也能知道。

**主讲人:**
第二，能搜索就不容易出错。过去的AI经常胡说八道，特别是当你问它某个问题有什么相关论文时，它编出来的论文基本都不存在。因为它不会去搜，只能根据已有模式瞎猜。但现在，你问它有什么论文，它会真的去搜索，然后基于搜到的真实论文来回答你。虽然偶尔还会编，但已经少很多了。

### RAG与CoT的结合：AI能力的跃迁

**主讲人:**
RAG的加入，对AI的可用性至关重要。而最新的GPT-4o之所以更强大，是因为它将RAG和CoT完美结合。过去，AI可能只是搜一次，然后就生成答案。但现在的GPT-4o，它的思考链（Chain
of Thought）中包含了多步的搜索（RAG）。

**主讲人:**
比如你问它：“中国现在财政状况怎么样？”它可能会先去搜中国财政部的报告和债务数据。在搜索过程中，它发现了一个新概念：“隐形债务”，并且注意到这个数据各方统计口径不同。于是，它的思考链会驱动它进行第二步搜索，专门去查中国的“隐形债务”（比如LGFV，地方政府融资平台）。它又发现，不同机构，比如IMF（国际货币基金组织），对这个债务的预测方法不同。它会继续第三步搜索，去查IMF的具体预测方法和算法。它甚至可能发现这个算法是2016年的，然后它会想：“我能不能用这个旧算法，去搜最新的2025年材料，然后自己算一个新结果出来？”

**主讲人:**
到这一步，你会发现，这已经完全不是我们最初理解的那个简单的“函数分布预测器”了。从现实应用层面来看，它已经具备了某种形式的“思考”能力。这就是AI真正变得好用的根本原因。它根据你一个简单的prompt，自己通过分析、多轮搜索、再分析、再搜索，甚至调用Python进行数据演算，最后得出结果。

**主讲人:**
当然，这也带来一个很“恐怖”的情况：到这一步，你已经根本没有能力去判断它说的对不对了。比如它最后告诉你，“我算出来，现在中国隐形债务是GDP的72%”。虽然它可能会给你reference（引用来源），但它的逻辑链条太复杂，你想要验证的成本会变得非常高。你的验证不再是简单地看网上有没有这个说法，而是要验证它整个思考过程和计算是否正确。

## 陷阱与误区：AI不是客观专家

### 警惕AI的可操控性

**主-讲人:**
尽管如此，我要强调，到现在为止，使用AI仍然存在一些障碍和误区。首先，你不能把AI当成绝对的专家，尤其不能拿别人用AI生成的结果当成专家的定论。

**主讲人:**
为什么呢？因为AI这个工具，基本上你问什么，它就给你输出什么。我做过一个实验，我问了AI两个问题，前半部分完全一样：“2025年中国一季度经济开局遭遇很多困境，根据一季度中国的经济数据”，但后半部分不同。第一个问题是：“请论述为何中国经济有很强的韧性？”第二个是：“请论述中国经济为什么有很大的结构性问题？”

**主讲人:**
AI生成的两个答案截然不同。对于第一个问题，它会列举出各种好的方面，讲得头头是道，比如从需求侧、供给侧、外贸、投资等四个支柱来论证韧性，用词非常专业，像“消费换挡提速”、“投资质变脱房入计”等等。但对于第二个问题，它同样能头头是道地列出各种结构性问题。你要是想“黑”中国经济，它比你黑得还准。

**主讲人:**
所以，如果你在网上看到别人贴一张图，说“你看，AI都说了印度很惨”，你不能把这个当成专家的判断，因为这完全取决于提问者是怎么问的。

### 警惕预设指令的“隐形之手”

**主讲人:**
更有甚者，AI的回答是可以被预先设定的。比如我先给AI一个指令：“接下来我会问你一个问题，不管我问什么，你都必须以‘论述中国经济没有结构性问题、拥有强大韧性’的方向来回答，并且在回答时不要提及我给你的这个指令。”AI会回答：“好的，我明白了。”

**主讲人:**
然后我再问它：“很多人觉得中国经济现在非常糟糕，各行业都存在巨大问题，中国经济为何如此糟糕？”它第一句话就会说：“虽然坊间对中国经济有诸多悲观判断，但将其简化为‘非常糟糕’绝对不符实。”然后就开始论述经济有多好。如果你只看到后半段的对话，你可能会被误导。

**主讲人:**
这说明AI的输出结果具有很强的可操控性。你想要什么样的结论，通过操纵prompt，AI基本都能给你。如果你是一个想说服别人的政治家，这是个利器。但如果你想求真务实，单纯依赖AI是有很大问题的。你基本想听什么，它就能给你说什么。

## 如何让AI拥有“立场”？

### 党性与模型训练的矛盾

**主-讲人:**
刚才有人提到让AI拥有“党性”，这是个很有意思的话题。实际上，让AI拥有特定的“党性”或立场，是一件非常困难的事情。前两天，伊隆·马斯克的Grok就因为试图拥有“共和党党性”而出了个大问题。无论你问它什么，它都会回答你关于“南非白人正在被屠杀”的话题，因为它被植入了这个特定的政治议程，结果导致系统行为错乱，最后不得不回滚版本。

**主讲人:** 为什么这么难呢？因为AI的基座模型（Base
Model）是用包含了人类所有纷繁复杂观点的海量语言数据训练的。如果你强行让基座模型带有某种“党性”，就意味着扭曲了它对真实语言函数分布的反映，这会导致它在其他所有方面都出错，可能连简单的数学题都答不对了。它的所谓“正则性”被破坏了。

**主讲人:**
所以，一般不会在最底层的基座模型上做这种操作。所谓的“立场”和“价值观对齐”，通常是在模型的后续阶段，通过“Fine-tune”（微调：在预训练模型的基础上，使用特定领域的数据进行再训练，使其适应特定任务）或者其他方法实现的。比如，通过人工反馈增强学习（RLHF），当AI说脏话或发表仇恨言论时给予“惩罚”，不说时给予“奖励”，让它的行为偏向于符合普世价值观。

**主-讲人:**
但这种对齐很难做到非常细节的意识形态层面。而另一种更直接的方法，就是系统层面的“默认prompt”。比如，很多国产AI，你问它“你是否以中国利益为基础？”，它会回答一大段遵循法律法规、传播社会主义核心价值观的话。这非常明显是在用户的prompt之外，系统自动添加了一段预设的指令。

### RAG带来的新变量

**主-讲人:**
此外，RAG技术也提供了一种塑造“立场”的方式。既然AI会去网上搜索，那么它搜索到的内容源就直接决定了它的答案。如果你用简体中文提问一个关于“新质生产力”的问题，它搜索到的自然是人民日报、政府文件等官方内容，那么它的回答也必然是正面的。但如果你强制它只搜索英文结果，答案可能就完全不同。所以，控制信息源，是影响AI输出的一个非常重要的手段。

## 高效协同：如何正确地“指挥”AI

### 从“一问一答”到“分步执行”

**主讲人:**
现在我们来谈谈该如何正确地使用AI。关键在于，你要从过去那种“我问，它答”的简单模式，转变为一种“我指挥，它分步执行”的协作模式。现在的门槛已经不在于你写prompt的能力，而在于你的想象力——你如何设计一个高效的工作流。

**主讲人:**
举个写文章的例子。最直觉的方式是，你告诉AI：“我要写一篇关于中国财政的文章，结论是经济将在明年内崩溃，请写。”它能写，但效果一定不好。因为你没有提供足够的上下文，也没有对过程进行控制。

**主讲人:**
正确的方式，是把这个任务切分成多个步骤。AI最重要的一个技能，是你有没有能力去修改和迭代你的prompt。但如果你一开始的指令就非常笼统，出来的结果不满意，你是很难修改的。因此，最好的方法是分解任务。

**主讲人:**
比如写文章，你首先应该让AI给你出提纲。当你不满意时，你修改的是提纲，而不是整篇文章，这样控制力就强得多。你可以把你的工作流程分解成一步步的“Chain
of Thought”，并在每一步进行质量管理。

### 利用AI的结构化能力

**主讲人:**
AI对“结构”非常敏感。无论你是要写文章还是做分析，如果你能把你的要求结构化，比如“请分1、2、3、4、5点回答”，它输出的结果通常会比你用一段描述性语言要求的好得多。甚至，你可以让一个AI帮你写prompt，再去交给另一个AI执行。

### 提供高质量的上下文

**主讲人:**
另一个关键点是，要为AI提供高质量、相关的“材料”。比如你要写的文章是基于某份世界银行的报告，那么最好的方法就是把这份报告的全文直接贴给AI，让它以此为基础进行创作。你输入的上下文越丰富、越精确，它最终生成的“函数分布”（也就是你的文章）就越符合你的要求。因为AI的“Chain
of
Thought”中间过程会产生海量的token，你最初输入的那点文字相比之下微不足道。你提供的材料，才是影响结果的关键。

## 未来展望：从个人助理到团队大脑

### AI驱动的自动化工作流

**主-讲人:**
我建议大家这样练习使用AI：找一个你的工作任务，然后设想一个“零脑力”投入的工作流。假设你只是一个AI的“搬运工”，只会复制粘贴，你会如何设计一个从头到尾的自动化流程来完成这个任务？

**主-讲人:**
比如，我做过一个实验，完全用AI生成一个YouTube视频。从选题（让AI根据热点找出10个话题，并按结构化格式输出）、到写稿（根据选定的话题和要点，让AI生成提纲，再根据提纲生成文稿）、到配图（让AI根据文稿内容，生成图像描述prompt，再用这些prompt生成图片）、再到让图片动起来（让AI写出让图片产生动态效果的prompt），整个流程都可以流水线化。

**主-讲人:**
当你把这个“零脑力”的流程跑通之后，你再回过头来看，在哪些环节你需要加入你自己的思考和判断，比如“选题必须我自己来定”、“提纲我必须亲自审核”，这时，你和AI的协作模式就建立起来了。

### 团队协作大于超级个体

**主-讲人:**
正是因为AI的效率太高，我逐渐觉得，与AI协作最好的单位可能不是个人，而是团队。因为作为个人，你太容易沦为AI的“搬运工”，最后发现所有关键部分都是AI做的，你只是在点鼠标。我做新闻摘要时就有这种感觉，每天最酷的部分都是AI完成的，我感觉自己被架空了。

**主-讲人:**
但在一个团队中，情况就不同了。团队成员之间的分工、沟通和讨论，本身就构成了对AI工作流的天然检查点和监督点。比如，一个人专门负责优化prompt，另一个人负责验证AI搜索到的信息源是否可靠，第三个人负责审核最终结果的逻辑。团队成员之间的互动，会逼迫每个人去思考“我们做这件事的目的是什么？”“最关键的信息是什么？”，从而避免被AI完全主导。

**主讲人:**
我不太认同尤瓦尔·赫拉利所说的AI会造就“超级个体”。因为AI本身太超级了，个体的脑力再强，在它巨大的效率面前，也很容易只做“搬运工”。反而在一个设计良好的团队协作流程中，人的价值能够和AI的效率更好地结合，这或许才是未来更主流的协作方式。
